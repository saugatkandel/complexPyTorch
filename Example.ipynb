{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear, ComplexConv2dGauss\n",
    "from complexPyTorch.complexLayers import ComplexDropout2d, NaiveComplexBatchNorm2d\n",
    "from complexPyTorch.complexLayers import ComplexBatchNorm1d\n",
    "from complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_train = 1000\n",
    "n_test = 100\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "train_set = datasets.MNIST('../data', train=True, transform=trans, download=True)\n",
    "train_set = Subset(train_set, torch.arange(n_train))\n",
    "test_set = datasets.MNIST('../data', train=False, transform=trans, download=True)\n",
    "test_set = Subset(test_set, torch.arange(n_test))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ComplexNet, self).__init__()\n",
    "        self.conv1 = ComplexConv2dGauss(1, 10, 5, 1)\n",
    "        self.bn2d  = ComplexBatchNorm2d(10, track_running_stats = False)\n",
    "        self.conv2 = ComplexConv2dGauss(10, 20, 5, 1)\n",
    "        self.fc1 = ComplexLinear(4*4*20, 500)\n",
    "        self.dropout = ComplexDropout2d(p = 0.3)\n",
    "        self.bn1d  = ComplexBatchNorm1d(500, track_running_stats = False)\n",
    "        self.fc2 = ComplexLinear(500, 10)\n",
    "             \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x.type(torch.cfloat))\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = self.bn2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = complex_relu(x)\n",
    "        x = complex_max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1,4*4*20)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = complex_relu(x)\n",
    "        x = self.bn1d(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.abs()\n",
    "        x =  F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ComplexNet().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target =data.to(device).type(torch.complex64), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train\\t Epoch: {:3} [{:6}/{:6} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                loss.item())\n",
    "            )\n",
    "            \n",
    "def test(model, device, test_loader, optimizer, epoch):\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device).type(torch.complex64), target.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Test\\t Epoch: {:3} [{:6}/{:6} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data), \n",
    "                len(test_loader.dataset),\n",
    "                100. * batch_idx / len(test_loader), \n",
    "                loss.item())\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skandel/.conda/envs/torchdiff/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\t Epoch:   0 [     0/  1000 (  0%)]\tLoss: 2.594220\n",
      "Test\t Epoch:   0 [     0/   100 (  0%)]\tLoss: 0.482387\n",
      "Train\t Epoch:   1 [     0/  1000 (  0%)]\tLoss: 0.916131\n",
      "Test\t Epoch:   1 [     0/   100 (  0%)]\tLoss: 0.402028\n",
      "Train\t Epoch:   2 [     0/  1000 (  0%)]\tLoss: 0.608397\n",
      "Test\t Epoch:   2 [     0/   100 (  0%)]\tLoss: 0.198328\n",
      "Train\t Epoch:   3 [     0/  1000 (  0%)]\tLoss: 0.130472\n",
      "Test\t Epoch:   3 [     0/   100 (  0%)]\tLoss: 0.022836\n"
     ]
    }
   ],
   "source": [
    "# Run training on 4 epochs\n",
    "for epoch in range(4):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "imag is not implemented for tensors with non-complex dtypes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run training on 4 epochs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test(model, device, test_loader, optimizer, epoch)\n",
      "Cell \u001b[0;32mIn [32], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m data, target \u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcomplex64), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/torchdiff/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_forward_pre_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers a forward pre-hook on the module.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m    The hook will be called every time before :func:`forward` is invoked.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m    It should have the following signature::\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m        hook(module, input) -> None or modified input\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m    The input contains only the positional arguments given to the module.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    Keyword arguments won't be passed to the hooks and only to the ``forward``.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m    The hook can modify the input. User can either return a tuple or a\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    single modified value in the hook. We will wrap the value into a tuple\u001b[39;00m\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;124;03m    if a single value is returned(unless that value is already a tuple).\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks)\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "Cell \u001b[0;32mIn [32], line 14\u001b[0m, in \u001b[0;36mComplexNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m complex_relu(x)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m complex_max_pool2d(x, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torchdiff/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_forward_pre_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RemovableHandle:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Registers a forward pre-hook on the module.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m    The hook will be called every time before :func:`forward` is invoked.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m    It should have the following signature::\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m        hook(module, input) -> None or modified input\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m    The input contains only the positional arguments given to the module.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;124;03m    Keyword arguments won't be passed to the hooks and only to the ``forward``.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03m    The hook can modify the input. User can either return a tuple or a\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;124;03m    single modified value in the hook. We will wrap the value into a tuple\u001b[39;00m\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;124;03m    if a single value is returned(unless that value is already a tuple).\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03m        :class:`torch.utils.hooks.RemovableHandle`:\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m            a handle that can be used to remove the added hook by calling\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;124;03m            ``handle.remove()``\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     handle \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mRemovableHandle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks)\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks[handle\u001b[38;5;241m.\u001b[39mid] \u001b[38;5;241m=\u001b[39m hook\n",
      "File \u001b[0;32m~/code/complexPyTorch/complexPyTorch/complexLayers.py:541\u001b[0m, in \u001b[0;36mComplexConv2dGauss.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    539\u001b[0m i_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mimag\n\u001b[1;32m    540\u001b[0m w_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mreal\n\u001b[0;32m--> 541\u001b[0m w_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimag\u001b[49m\n\u001b[1;32m    542\u001b[0m b_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m    543\u001b[0m b_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mimag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: imag is not implemented for tensors with non-complex dtypes."
     ]
    }
   ],
   "source": [
    "# Run training on 4 epochs\n",
    "for epoch in range(4):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.complex64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cfloat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Defines the computation performed at every call.\n",
       "\n",
       "Should be overridden by all subclasses.\n",
       "\n",
       ".. note::\n",
       "    Although the recipe for forward pass needs to be defined within\n",
       "    this function, one should call the :class:`Module` instance afterwards\n",
       "    instead of this since the former takes care of running the\n",
       "    registered hooks while the latter silently ignores them.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.conda/envs/torchdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l1.forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Conv1d' has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ml1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Conv1d' has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "l1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdiffeq",
   "language": "python",
   "name": "torchdiffeq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
